---
layout: post
title:  "Learning Machine Learning"
date:   2017-05-15 11:00:00 -0700
categories: Machine Learning
---

# Learning Machine Learning
<p style="text-align: justify"> 
On my recent travels, while browsing the local newspaper over a cup of a local flavor of coffee, an article in the paper re-ignited my interest in Machine Learning. Recent innovations in machine vision, self-driving vehicles, image and speech recognition,  have spurred a flood of intrigue/interest in ML, including mine. Over the past few years, amongst other things, I had also worked in “knowledge” related fields. At one time, knowledge worker services meant providing email, calendaring and documents over the net. Despite the sweeping title of that project , ever since I worked on it,  my interest and intrigue in “knowledge based services” had caught on. In the recent past I had struggled to codify knowledge and let reasoners loose on  modeled data with tools and techniques from the fields of Knowledge Engineering (like  Ontological dbs, OWL/RDF, Semantic Knowledge Representations-SKRs, Reasoners, UIMA , Semantic Web,  etc..) The ‘simple goal’ was to “glean knowledge” from text (technical  papers), to separate the chaff from the grain in the jargon, as a human would do (i.e information extraction). Disappointed as I was with the rigor and rigidity of some of the frameworks, and the complexity of the coding involved, I was not progressing very far. The “self-programming and hands-off learning” paradigm of ML seemed promising with amazing advances in key fields including  image recognition, language processing, autonomous driving etc., So, I set about “Learning Machine Learning”. Looked at the task at hand (title) in  two ways..”Learning, Machine-Learning”.. or .. “Learning-Machine, Learning”.. I decided on the latter approach.. design a machine or two to learn, sit back and relax..and let the machines do all the learning about ML !! How cool would that be ?!?  ( We will digress briefly, meander a bit  and get back to this task..but if you want to jump into the code right away , [here it is](https://github.com/rmandyam/ml-rnn-text-generation) )
</p>

## The Master Algorithm
<p style="text-align: justify">
In his book The Master Algorithm, author Prof. Pedro Domingos catalogs the five major tribes of ML as follows - Symbolists, Connectionists, Evolutionaries, Bayesians and Analogizers. Each of these schools of thought in ML are inspired by their specific domains of knowledge. Symbolists are inspired by Philosophy, Psychology and Logic and view learning as the inverse of deduction. Connectionists are inspired by Neuroscience and Physics and try to achieve learning by reverse engineering Neural Net like architectures in the Human Brain. Evolutionaries are inspired by Genetics and Evolutionary Biology and try to accumulate knowledge thru learning by simulating Evolution in the digital realm of a computer, ‘in silico’, rather in ‘in vivo’. Bayesians rely on Statistics to guide their learning architectures thru Probabilistic Inference. Analogizers are inspired by Psychology and Mathematical Optimization and learn by extrapolating from similarity judgements. 
</p>


<p style="text-align: justify">
Also, each of these five schools of thought has it’s own Learner, the Master Algorithm that “gleans Knowledge from Data” in any domain. For the Symbolists  the main Learner is Inverse Deduction, for the Connectionists, it’s BackPropagation, the Evolutionaries, it’s Genetic Programming, the Bayesians’ it’s Bayesian Inference, and for the Analogizers it’s the Support Vector Machine. 
</p>
<p style="text-align: justify">
 The book is a fascinating read to get a good understanding of these different schools of thinking in ML.
</p> 

## Brain, Speech, Language and Learning
<p style="text-align: justify">
For a variety of reasons, I have been interested in the human brain, speech and languages. As I have been reading up on the macro, micro and cyto  architecture of the Brain, Mammalian Vocalization and language processing, I am constantly reminded that as humans we are continuously learning and there is so much ever to learn,  as we keep pushing the frontiers of knowledge beyond the perimeters of ignorance.  Also as we raise kids in today’s knowledge rich world, it behooves us all to be armed with every important theory, tool and technique in human learning. 
</p>

<p style="text-align: justify">
It was just a few hundred years ago, in the Western world,  that Mankind was dislodged both biologically and physically from the “Center of the Universe” (scientifically speaking) . It’s been a few hundred years since and less than a hundred years ago, that we have made tremendous strides in our learning and thinking. We have rapidly shifted our stance from a Newtonian to an Einsteinian and Quantum framework. We discovered DNA and have gone from Genes to Genomes. We have discovered other natural worlds and systems  beyond our home galaxy and figured out the extent  of our expanding Universe beyond galaxies. We have figured out the age of our Universe, come up with theories for the origin of ours and possibly of others within a Multiverse. And all of this,  we have achieved with the greatest machine of all that has been built in this Universe ( atleast in the known parts ). We all possess this machine right between our ears. We have put a whole universe in our Brain. The possession of this unique apparatus puts us back at the pinnacle of success in this Universe. The Descent and Ascent of Man, so to say , within a short span of geologic time (again, from a scientific perspective)  . For these reasons alone, the Brain makes a fantastic subject of study. 
</p>

<p style="text-align: justify">
It’s for the human connection,  that I was drawn towards Connectionism and Neural Nets(NNs) . NNs try to mimic neuroscience , but going the other way,  NN’s have been used in cognitive modeling and have been considered complementary to techniques in neuroscience. Their potential in helping understand Brain and cognitive processes is only limited by available computational and algorithmic resources and these frontiers of knowledge are expanding every day. 
</p>
<p style="text-align: justify">
Before we dive into Machine Learning, first a word on human learning, speech and language. The human capacity for Speech and Language is of considerable mystery. Human learning and speech are  complex phenomena and despite the mystery,  we all seem to do it with ease. That’s because we all have been endowed (evolutionarily or otherwise) with the greatest statistical ML of all time. Humans make it look easy. We can learn things, good and bad, very easily (of course, under the right conditions). Many environmental factors are implicated in human learning among which culture, society, demographics, politics, economics and nutrition seem to play leading roles. Also, good intent, bad distractions, motivation and boredom seem to affect the learning rate. Human learning is also multi-modal. All five senses can be employed to enrich the learning experience. 
</p>
<p style="text-align: justify">
Speech is a fundamental characteristic of humans that sets them apart from other denizens of the planet. For a variety of reasons I have been interested in, Speech, Languages and  their structure and semantics to convey knowledge and power in thought and action.  Speech and Language have the power to shape and build thoughts, societies, culture and civilizations. As humans we learn a lot and we learn in a variety of ways. One important channel of learning is thru a language either verbally, visually, written, signed or felt. Language is one of  our biggest gateways to knowledge. In many older cultures, you learn more than one language. As humans we are exposed to these languages throughout our lives and the neural networks in our brains are fine tuned thru the formative years.  
</p>
<p style="text-align: justify">
So, combining the above topics of interest, I set about exploring ML with Neural Nets and Natural Language Processing (NLP). (?)Offline, I have read several books and papers (?).  Online, there are a variety of resources that I have read, looked at, listened to, viewed, adapted and learnt from. Also, there are a variety of frameworks, APIs, languages, platforms, libraries, tools to implement ML in. And, then there are a variety of problems, architectures and solutions to work on and explore in  ML. Personally, for my learning, I wanted a consistent way of looking at ONE example (text/sentence generation ) through the different platforms (TF, DL4J and Theano with Python/Java). &.. Use this as a learning aid.. Before, we look at the implementation here is another brief digression. 
</p>

## Neural Networks
<p style="text-align: justify">
Here is a brief note on Neural Networks(NNs). As mentioned before, NNs are Connectionist models and they mimic a network of neurons in the brain. A simple model of a biological neuron has a number of incoming inputs. Each input is “assessed” i.e. weighted and the neuron “sums” up these weighted inputs and if this exceeds a “threshold” (bias) , the neuron fires and produces an output. Many neurons co-operate to work on tasks within the brain. These co-operating neurons coalesce into networks following the Hebbian rule “neurons that fire together, wire together” (loosely stated, ignoring causality i.e spike temporality).  Neural Networks ( Artificial Neural Networks, ANNs) are similar. Neurons in the network have trainable weights and biases. The summation in each neuron is carried out by an Activation Function like the sigmoid, tanh or ReLu (rectified linear unit) functions. “Learning” in the NN occurs when the weights and biases are fine tuned towards a goal/task. Fine tuning of the weights and biases is accomplished by Back-Propagation algorithms. Minimizing the error in learning or determining the direction to proceed in fine tuning is accomplished by algorithms like Gradient Descent (Stochastic Gradient Descent, SGD etc..) which try to descend towards a minima on the hyper-dimensional error/loss surface, thereby  reducing the error in learning. The error itself ( i.e loss or the divergence from a stated goal or task) is quantified by a Cost / Loss or Error function. MSE ( mean square error) and Cross Entropy are some useful concepts in defining cost functions. While descending the error/loss surface, the battle between getting stuck in a local minima versus finding the optimal or global minima leads to Optimization algorithms for improved learning rates and performance. 
</p>
<p style="text-align: justify">
NNs consist of a input layer, one or more hidden layers and an output layer. In a Feedforward NN , data is fed in, processed in each hidden layer and an output is produced. In some networks, the output thus produced is fed back in as input after a short interval of time. Thereby introducing the dimension of time into processing and establishing a feedback loop. These are Recurrent Neural Networks or RNNs. They are like Feedforward networks unfurling in time steps and hence being  ideal for processing sequences of data. There are many other variants of NNs beyond the scope of the current context. For e.g Convolutional Neural Networks (CNNs) is a popular choice for image processing.
</p>
<p style="text-align: justify">
Going back to the “learning” process in NNs, for a network to learn smoothly and make consistent and continuous  progress,  the “process has to be continuously differentiable”, or the “error flow has to be continuous”. “Plain vanilla” network architectures like RNNs are beset with problems like “vanishing gradients” or “exploding gradients”  whereupon the learning is completely stymied after a while (short number of sequential steps). Thanks to architectural model improvements like Long Short-Term Memory (LSTM)  and Gated Recurrent Units (GRU) . These can provide “stable gradients” and “continuous error flow” .Thereby enabling the handling of longer sequences much more efficiently and hence providing tremendous improvement in network learning. Further improvements/innovations in network architecture models have led to incorporating Attention and Memory mechanisms for certain tasks like reasoning. DMNs (Dynamic Memory Networks) for e.g provide improved learning/reasoning in Facebook bAbI tasks. 
</p>
<p style="text-align: justify">
Training a NN itself involves three major learning paradigms. Supervised Learning with labeled features in the training data, Unsupervised Learning  where you elicit hidden structure from unlabeled training data  and Reinforcement Learning using ideas from behavioral psychology and a rewards mechanism. The constellation of numbers generated at the end of training a NN are both staggering and  a bit mysterious. Nonetheless, the results are impressive.
</p>
<p style="text-align: justify">
Computationally, the nodes and trainable parameters ( weights and biases ) in a NN are represented by multi-dimensional arrays or matrices, specifically Tensors. To simplify conceptual, symbolic and computational representation, Vectors are used. That’s it, we are almost ready to grapple with the implementation of a NN. 
</p>

## Sequence Learning
<p style="text-align: justify">
Now, finally,  to the task at hand. NLP offers many interesting challenges with speech and text. Speech recognition, Sentiment Analysis, Image captioning, Machine Translation , Handwriting Recognition,  Speech to Text and Text to Speech are a few. For my exploration of ML, I have chosen Text Generation (Sentence generation)  as an excercise. My goal is to design a learner(s) and set  it  (them)   loose learning Machine Learning by munching on words from technical papers about ML. Then after it’s  done learning, the true test of it’s “knowledge” comes when it starts spitting  out sentences as if writing a tech paper on ML all by itself. 
</p>
<p style="text-align: justify">
I cannot let this opportunity slide to contemplate on the recursive nature of this exercise of a NN learning about ML. But to begin with,  NN with ML were modeled after learning by neural networks in humans !! At one level, we live in a very fractal world. Not only that, we also live in a multi-dimensional world. Apart from the dimensions of the space-time continuum, we look at things from many perspectives, each a mental dimension of it’s own. This multi-dimensional framework within the brain can be thought of as akin to the hyper-dimensional learning or error/loss surface in a NN. 
</p>
<p style="text-align: justify">
The particular example of text (sentence) generation that I have chosen, falls under the category of “Sequence Learning” in ML. As is abundantly explicit in available technical literature, RNN’s are the Connectionist model of choice for Sequential tasks. Though reading the literature suggests that sequential learning with RNN is largely a heuristic and empirical effort, it’s re-assuring to know that it’s backed by some solid math, architecture, algorithms, analyses, reflection, intuition and good results. This heuristic and empirical nature of learning is complicated by the fact that it is hard to find the global minima on the hyper-dimensional error/loss surface (and the optimization of the error/loss is known to be a NP-hard problem, whatever that means, not simple !!). Also, this nature makes it a little like raising kids. A lot like it. Like training and initializing with a lot of exemplars of the right kind. The nourishing care has to be provided thru hyper parameter fine tuning and good curated examples.
</p>
<p style="text-align: justify">
As mentioned earlier in the goal, the input to the RNN model is a collection of technical papers on ML. More specifically, the inputs are words from sentences in the papers.  These words are represented as vectors, either one-hot, or infused with some “meaning” from a meaning vector  space based on the Distributional Hypothesis in Language Modeling which states that words that appear in the same contexts share semantic meaning. GloVe and Word2Vec are two freely available algorithms that produce word vectors from statistical co-occurences of words in a given context.  To reduce the size of the computation, a limit on the vocabulary is set. 
 </p> 
<p style="text-align: justify"> 
The NN architecture consists of a set of input nodes (neurons) the size (number) of which is usually the size (length) of the word vector. The size of the hidden layer and number of layers varies with the NN model chosen ( either RNN, LSTM or GRU ).  The output is a probability distribution over the target vocabulary. This is computed by a softmax non-linearity in the output layer of nodes equal to the size of the target vocabulary .
</p>

<p style="text-align: justify">
The Learning itself (i.e the adjusting of weights and biases in the model) happens through calculating and optimizing the “loss” thru BPTT (backpropagation through time ) and SGD algorithms (as stated before). One term used to define loss is cross-entropy. This signifies the divergence between the predicted probabilistic outcome and the targeted outcome. Reducing cross-entropy indicates better learning outcomes.
</p>

## The Code
<p style="text-align: justify">
A note on the code. The accompanying code provides the implementation using three different platforms/libraries ( TensorFlow, DL4J, Theano) with two different languages ( Python, Java).  Also, three different NN models are used. RNN, LSTM and GRU.  As a learning aid, these combinations should provide ample variation and nuances in modeling, architecture , implementation, language and choice. Also,the same input data is treated similarly across all examples to keep focus on the core of the ML implementation under each paradigm. All, while focusing on the same problem of text generation in NLP. (Note : As API’s are evolving and sometimes lack clear full fledged documentation, use the code as a learning aid and see if the accuracy and performance can be improved  by using alternate methods from the APIs, if they exist).
</p>

<p style="text-align: justify">
For those interested in cloud computing, All of the above computation can be extended into (implemented in)  the Cloud, with “CloudML” platform offerings from Google, Amazon and Microsoft.
</p>

## Sample Generations
<p style="text-align: justify">
So, how did my learners do ? My goal was to code a NN learner, that would read papers on ML and start to write such papers. While the goal was lofty and I was severely constrained for compute resources, the NNs did output some gems. Here are a few.  As, you can tell, they have a ways to go technically and grammatically, but atleast they recognize who the significant players in the field are !!
</p>

>----- Sample 0 ——  
nonzero or mrnn task both sequences are paper update problems Schmidhuber least following does very which and the trying also

>----- Sample 24 ——  
we machine the local regularities for phrases of or using a by of of so of further below symbol on

>----- Sample 18 ——  
same generated g that of weights from that paper and see analysis that sequence rnn applications even can language or


>----- Sample 18 ——  
rnn their are matrix we as neural table a rcc recurrent model many rnn product we given they gate proposed

>----- Sample 23 ——  
y oc and a applications from standard = training is f sutskever original for sequence was s but was shows

>----- Sample 23 ——  
euw variable-length turing-completeness anti-clockwise informatik kag vs senior ycjt riemann yoshizawa nwmax jobs m=q interestingly gigaword induction deterioration syntax kh

>----- Sample 24 ——  
switzerland intuitions signs mechanisms satisfy outer-product martens nlp \time variance wuv sg netinj feed expressing drawn hessian-free coco surprisingly yjt

>----- Sample 15 ——  
choose nonetheless almeida optimal surprisingly extra later creating hardly darpa vc intervals netcv altogether arrive extend boltzmann ftm performs wlv

>----- Sample 24 ——  
above bridge an and success raging where evolution hinton@cs resembles neural actions labelled on off chosen with wev where \internal

>----- Sample 16 ——  
l learning equality science reasonably ephemeral time distinguish euw notes easy stanford cause biasing other testing than per forward exploited

>----- Sample 7 ——  
phrases symbol is argument common well from which random sequential be rnn related similarly neural test in regularities approach by

>----- Sample 0 ——  
and may do encoder-decoder plausible special tensors models plot suggests s kalman deepest allows steps points indeed training testing plot similar

>----- Sample 12 ——  
n-gram together karpathy hf hidden-to-hidden co-occurrence learn respectively typically schuster language character-level gaussian metrics image back     propagated deep required extremely online

>----- Sample 24 ——  
encoder bidirectional martens karpathy mb reasonably program node collobert novel bh directed variable-length schuster sequence list differentiable wrong corpora bh

>----- Sample 14 ——  
x<t bridge indeed calculated describe socher n-gram ^ karpathy quickly programs read prediction lists evaluate smt markov backpropagated maximize knowledge

>----- Sample 22 ——  
variety last socher signals ht compare indices -dimensional character-level adjacent internal itself convey edu bengio help martens nseq nseq knowledge

>----- Sample 19 ——  
changes ka equal points variable-length character-level candidate x<t hence ka related impressive object karpathy variance symmetric variable-length socher impressive variety




Also, I did train the NN on the entire corpus of Sherlock Holmes. Here are some gems from that effort. Though the NNs have not coalesced to be coherent yet, they seem to be getting the Holmesian gist !!

>----- Sample 14 ——  
a watson high moors initials night great at too which to at one a when upon is of be down

>----- Sample 17 ——  
journal sportsman size lord sit swiftly certificate constructed property wifes struggle charge villains india smoked hanging whoever printed book manage

>----- Sample 14 ——  
protruded stains staggered indicated career confide laughter scandal preparation crowded muzzle fortnight loudly dream butler ive chinese fallen indeed everything

>----- Sample 10 ——  
policemen convinced strike hateful gang guide foreign sky companion ice rules factors douglas respectable groom missed smell preparing violently expedition

>----- Sample 6 ——  
complaint forcing literature disturbance begin lets theory spray saturday driven persons curiosity government startled policeman collected inquiry dainty says motionless

>----- Sample 0 ——  
inspect assurance answer de masterful american pinkerton mistress talents gap shape pushed relatives hes suffering evil loaded search protection problems

>----- Sample 1 ——  
preserve counter freemen events examination success higher domestic known doorway warrant indicate routine agreed stair key youre look indication carriages

>----- Sample 12 ——  
reading private packet flushed unexpected signal individuals war document eh drinking community add formidable horrified consent provided late dressing-gown thence

>----- Sample 13 ——  
tut scattered comic afoot exhibited singularly prosaic beast correspondence accused neither sheep sir inner among keeping steam carriage showing gorgiano

>----- Sample 14 ——  
nobody temptation arm chosen croydon faint neighbour strangers sum piece delirious horse exchanged died also mirror sleeve trap tobacco servant

>----- Sample 15 ——  
floor avenger electric highly whats bosom forbid guest grass talked destruction anyhow motive odds attend sitting divorce souls lithe treasure



>----- Sample 6 ——  
blackmail restaurant earn served initials alluded dressed fired havent drawer sullen murder personal killing finally workman brutal hunters pipes existence

>----- Sample 0 ——  
goods twinkle hit excitement detected gossip careless boss happens compared fresh history truly easy disturb utterly takes court bearing realize

>----- Sample 5 ——  
chance at to first way double results capable dont among cursed hound fairly across write only night he already he

>----- Sample 17 ——  
could more have an also was us just my all man at is do when watson what him was upon



## Afterword
<p style="text-align: justify">
Some of you who have read the blog this far, might be wondering, why I haven’t explored in great depth anything technical about NNs. Well, that’s one of the points. With the available resources online, you too can rapidly ramp up to be a machine learner. Sentence Generation is a start. With your own NNs you might one day be discovering newer models in the dynamics of politics, economics and society, or newer models in medicine helping discover new pathogens, new genetic mutations, drugs, patient-customized treatments or go beyond to discover new worlds beyond our galaxies. Who knows, one day your NN might pen a poem  or even write a technical paper on ML !!
</p>
<p style="text-align: justify">
As for me, I am still chugging along with my goal from previous years, that of rapidly extracting knowledge from written tech papers. As it turns out, information extraction, though simple to comprehend , is a hard problem waiting for an elegant solution.
</p>


## Acknowledgements
<p style="text-align: justify">
Many excellent articles and tutorials exist online that explore in depth the technical details of NNs. I would like to thank all of those authors for their painstaking efforts. I would also like to thank the Professors, Professionals, authors and aggregators  for their excellent videos on Deep Learning and NLP. While I may not have mentioned all of them, some of them are in the links below. The accompanying code is borrowed, adapted and modified from tutorials from their respective orgs, authors,   and repos available on github. Code from Denny Britz(WildML.com) , YerevaNN, and Alex Barron were of great help.
</p>

## References

### Books
1. Language and Mind  - Noam Chomsky
2. Making the Social World : The Structure of Human Civilization - John R Searle
3. The Infinite Gift - How Children Learn and Unlearn the Languages of the World -  Charles Yang
4. Self comes to Mind : Constructing the Conscious Brain  - Dr. Antonio Damasio
5. The Tell Tale Brain - A Neuroscientist’s quest for what makes us human - V.S. Ramachandran
6. The Master and his Emissary - The Divided Brain and the making of the Western World - Iain McGilchrist
7. The Brain that Changes itself - Norman Doidge
8. The Master Algorithm - How the Quest for the Ultimate Learning Machine will remake our World - Pedro Domingos

### Papers - Computer Science
1. Long Short-Term Memory Neural Computation  - Sepp Hochreiter , Jurgen Schmidhuber 
2. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation - Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio ( https://arxiv.org/abs/1406.1078 )
3. Generating Text with Recurrent Neural Networks - Ilya Sutskever, James Martens, Geoffrey Hinton
4. Global Vectors for Word Representation - Jeffrey Pennington, Richard Socher, Christopher D Manning
5. A Critical Review of Recurrent Neural Networks for Sequence Learning - Zachary C Lipton, John Berkowitz, Charles Elkan (https://arxiv.org/abs/1506.00019)
6. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling - Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, Yoshua Bengio - https://arxiv.org/abs/1412.3555
7. A Few Useful Things to know about Machine Learning - Pedro Domingos - https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf
8. Learning Distributed Representations of Concepts - Geoffrey E Hinton
9. Towards AI-Complete Question Answering : A set of Prerequisite Toy Tasks - 
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart van Merrienboer, Armand Joulin & Tomas Mikolov

### Papers - Neuroscience
1. The Neural Control of Vocalization in Mammals  : A Review - U. Jurgens
2. A Neural theory of speech acquisition and production - Frank H Guenther, Tony Vladusich
3. Conceptual Representations in Mind and Brain - Markus Kiefer, Friedemann Pulvermuller
4. Speech Production - Wernicke, Broca and beyond - S. Catrin Blank, Sophie K. Scott , Kevin Murphy, Elizabeth Warburton, Richard J. S. Wise 
5. The Speaking Brain - A  tutorial introduction to fMRI experiments in the production of speech, prosody and syntax		G Dogil, , H Ackermann, W Grodd, H Haider, H Kamp, J Mayer, A Riecker, D Wildgruber
6. Perception, action, and word meanings in the human brain: the case from action verbs - Bedny M , Caramazza A.
7. The Faculty of Language - What is it, Who has it and How did it Evolve ? - Marc D Hauser, Noam Chomsky,  W Tecumseh Fitch
8. On the Relation of Speech to Language - Alvin M Liberman, Doug H Whalen 

### Blogs/Online references
1. TensorFlow Tutorials <https://www.tensorflow.org/tutorials/>
2. DeepLearning For Java Tutorials <https://deeplearning4j.org/tutorials>, Core Concepts <https://deeplearning4j.org/core-concepts>, RNNs <https://deeplearning4j.org/usingrnns#basics>
3. Neural Networks and Deep Learning - Michael Nielsen - <http://neuralnetworksanddeeplearning.com/index.html>
4. The Unreasonable Effectiveness of Recurrent Neural Networks - Andrej Karpathy - <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>
5. Understanding LSTM Networks - Christopher Olah - http://colah.github.io/posts/2015-08-Understanding-LSTMs/
6. Recurrent Neural Networks Tutorial - Denny Britz - http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/
7. UW CS - Data Mining , Machine Learning -  https://courses.cs.washington.edu/csep546﻿ ,  Online course at  https://www.youtube.com/watch?v=ccMBRMDj9Qo&index=1&list=PLTPQEx-31JXjLuY5y6guK6MFmfkxy7YXv
8. TedxUofW - The Quest for the Master Algorithm - Pedro Domingos - https://www.youtube.com/watch?v=qIZ5PXLVZfo
9. Stanford NLP - CS224n - NLP with Deep Learning - http://web.stanford.edu/class/cs224n/
10. The bAbI Project - https://research.fb.com/projects/babi/
11. www.BayAreaDLSchool.org - Deep Learning for NLP - Richard Socher -  https://www.youtube.com/watch?v=oGk1v1jQITw&feature=youtu.be . Also, check out other speakers on  videos posted by Lex Friedman on YouTube 
12. Deep Learning for NLP (without magic) - Parts 1 and 2 - http://techtalks.tv/talks/deep-learning-for-nlp-without-magic-part-1/58414/

### Code/github repos : 
(While it’s useful to have a background in programming, there are plenty of online tutorials on Java and Python to get you started if you don't have one.)

1. tensorflow.org
2. deeplearning4j.org 
3. Stanford University - Assignment templates in cs224d/n course NLP with Deep Learning from  http://web.stanford.edu/class/cs224n/
4. Denny Britz - WildML.com - https://github.com/dennybritz/rnn-tutorial-gru-lstm
5. YerevaNN - https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano
6. Alex Barron - https://github.com/barronalex/Dynamic-Memory-Networks-in-TensorFlow

